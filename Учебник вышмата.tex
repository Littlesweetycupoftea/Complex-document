\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\numberwithin{equation}{section} 
\title{\Huge{\it Алфавитное кодирование\\
		Параграф 12.4 Блочное кодирование	}}
\date{}
\begin{document}
    \maketitle
    \newpage
    
    \section{Блочное кодирование}
    Пусть $P = {p_{1},...,p_{n}}$ — распределение вероятностей на алфавите
    $A$. Далее будем рассматривать кодирование блоками — будем кодировать слова длины $k$, составленные из букв алфавита $A$, полагая, что вероятности всех букв в каждом слове независимы, т. е. что равенство
    
     $$ p (a_{i1} \cdots a_{ik} ) = p (a_{i1} )\cdots p (a_{ik} ) = p_{i1} \cdots p_{ik} $$
     
справедливо для любого слова $ a_{i1} \cdots a_{ik} $ из $ A^{k} $. Распределение вероятностей на множестве $ A^{k} $ будем обозначать через $ P^{k} $. Пусть $ V^{k} $ — код множества $ A^{k} $.
    Величину
   
   $$ C_{k}(V_{k}, P) = \frac{1}{k} C(V_{k}, P^{k}) $$
   
    назовем средней стоимостью буквы алфавита $ A $ для распределения
    $ P $ при использовании кода  $ V_{k} $, а величину
    
    $$ C_{k} (P) = min_{V_{k}} C_{k}(V_{k},P) = 1\frac{1}{k} C(P^{k}) $$
    
— средней стоимостью буквы для распределения $ P $ при кодировании блоками длины $ k $. Ниже в теореме 12.6 устанавливаются оценки Шеннона для величины $ C_{k} $. Для доказательства этой теоремы потребуется имеющая самостоятельный интерес лемма об аддитивности энтропии независимых распределений.



{\bf Лемма 12.1.}{\it Пусть $ P = {p_{1},\ldots,p_{n}} $ и $ Q = {q_{1},\ldots,q_{m}} $ — независимые распределения вероятностей на алфавитах
$A = {a_{1},\ldots,a_{n}}$ и $ B = {b_{1},\ldots,b_{m}} $. Тогда для распределения вероятностей $ PQ $ на алфавите $ AB = {a_{i}b_{j}} $ справедливо равенство}

$$ H(P Q) = H(P) + H(Q). $$


\textsc{Доказательство.}
 
\begin{multline*}
 H(PQ) = -\sum_{ij} p_{i} q_{j} \log_{2} p_{i}q_{j} = -\sum_{i}\sum_{j}p_{i}q_{j} (\log_{2} p_{i} + \log_{2} q_{j} ) =\\ =-\sum_{i}( p_{i} \log_{2}p_{i} \sum_{j} q_{j}) -\sum_{i}( p_{i} \sum_{j} q_{j}\log_{2}q_{j} )=\\ =- (\sum_{i} p_{i} \log_{2}p_{i})(\sum_{j} q_{j}) - (\sum_{i} p_{i})(\sum_{j} q_{j}\log_{2}q_{j})= H(P)+H(Q)
 \end{multline*} 
Лемма доказана.


Из леммы 12.1 легко следует, что 
\begin{equation}
H(P^{k}) = kH(P) 
\label{first}
\end{equation} для любого распределения $ P $ и любого натурального $ k $.


{\bf Теорема 12.6.}{\it Для любого распределения вероятностей}
$ P={p_{1},\ldots ,p_{n}}$ $$H(p_{1}, \ldots ,p_{n}) \eqslantless Ck(p_{1},\ldots ,p_{}n) \eqslantless H(p_{1},\ldots ,p_{n}) + \frac{1}{k} .$$


\textsc{Доказательство}. Из равенства~\eqref{first} и теоремы 12.5 следует, что
\begin{equation}
kH(P) = H(P^{k}) \eqslantless C(P^{k}) \eqslantless H(P^{k})+1 = kH(P)+1.
\label{second}
\end{equation}

Так как $ C(P^{k}) = kC_{k}(P) $, то разделив все члены в~\eqref{second} на $ k $, получим требуемые неравенства для  $ C_{k}(P) $. Теорема доказана.\newline

Из теоремы 12.6 следует, что выбирая длину блока достаточно большой,
среднюю стоимость буквы можно сделать сколь угодно близкой к энтропии
распределения $ P $.
\end{document}